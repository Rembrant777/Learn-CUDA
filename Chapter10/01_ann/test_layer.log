Running main() from /home/raytrack/workspace/Learn-CUDA/Chapter10/googletest/googletest/src/gtest_main.cc
[==========] Running 8 tests from 2 test suites.
[----------] Global test environment set-up.
[----------] 3 tests from TestLayer
[ RUN      ] TestLayer.DenseLayerCreateTest
[       OK ] TestLayer.DenseLayerCreateTest (0 ms)
[ RUN      ] TestLayer.ActivationLayerCreateTest
[       OK ] TestLayer.ActivationLayerCreateTest (0 ms)
[ RUN      ] TestLayer.SoftmaxLayerCreateTest
[       OK ] TestLayer.SoftmaxLayerCreateTest (490 ms)
[----------] 3 tests from TestLayer (490 ms total)

[----------] 5 tests from Testlayer
[ RUN      ] Testlayer.DenseInitWeightBias
.. initialized dense-layer-0 layer ..
**bias-value	: (8) 	.n: 1, .c: 1, .h: 8, .w: 1	(h:0x26edee0, d:0x7fd76e600600)
<---- batch [0] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [1] ----->
	0.000000	0.000000	0.000000	0.000000	-0.000000	0.000000	0.000000	0.000000	
<---- batch [2] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [3] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
**weight-value	: (192) 	.n: 1, .c: 1, .h: 24, .w: 8	(h:0x26ee0b0, d:0x7fd76e600200)
<---- batch [0] ----->
	0.497185	0.432557	-0.371876	0.499041	-0.263911	-0.103419	-0.112089	0.169746	0.435539	0.346311	-0.186726	0.024548	-0.056547	-0.270423	0.034414	0.413962	
	-0.042795	-0.069301	0.439128	0.278389	0.215971	0.302758	-0.407199	0.018153	0.365020	0.329147	0.329603	-0.226950	-0.440757	0.170528	0.093066	0.171654	
	-0.088212	-0.302449	-0.210370	-0.357880	0.283314	-0.087461	-0.465829	0.124030	0.160636	-0.201505	-0.053865	-0.277875	-0.426636	-0.030761	-0.403828	0.403370	
	-0.380510	0.024799	-0.416377	0.416861	0.410448	-0.201070	0.084389	0.065912	0.113938	0.456536	-0.239021	-0.268985	0.033448	0.449938	-0.006940	0.040601	
<---- batch [1] ----->
	0.000000	0.000000	0.000000	0.000000	-0.000000	0.000000	-0.000000	0.000000	0.000000	0.000000	17332973938724745792349798400.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	-0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	75656244786851946737872928768.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	77130347389410020796293214896128.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [2] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	-0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	17331160549995323848587739136.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [3] ----->
	-0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	14855280471424563298789490688.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
.. saving dense-layer-0 parameter .. done ..
dense-layer-0load_parameter
.. loaded dense-layer-0 pretrain parameter..
**layer1-weights	: (192) 	.n: 1, .c: 1, .h: 24, .w: 8	(h:0x2703550, d:0x7fd76e600a00)
<---- batch [0] ----->
	0.497185	0.432557	-0.371876	0.499041	-0.263911	-0.103419	-0.112089	0.169746	0.435539	0.346311	-0.186726	0.024548	-0.056547	-0.270423	0.034414	0.413962	
	-0.042795	-0.069301	0.439128	0.278389	0.215971	0.302758	-0.407199	0.018153	0.365020	0.329147	0.329603	-0.226950	-0.440757	0.170528	0.093066	0.171654	
	-0.088212	-0.302449	-0.210370	-0.357880	0.283314	-0.087461	-0.465829	0.124030	0.160636	-0.201505	-0.053865	-0.277875	-0.426636	-0.030761	-0.403828	0.403370	
	-0.380510	0.024799	-0.416377	0.416861	0.410448	-0.201070	0.084389	0.065912	0.113938	0.456536	-0.239021	-0.268985	0.033448	0.449938	-0.006940	0.040601	
<---- batch [1] ----->
	-nan	-nan	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	17335391790363975050699210752.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	-0.426636	-0.030761	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	-0.001891	-0.426208	0.000000	0.000000	
<---- batch [2] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	17333583124001035976582365184.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [3] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	-0.000000	0.000000	-8059650260166869525640153667697901568.000000	0.000000	0.000000	1048785.000000	
	0.000000	68727930880.000000	0.000000	68727930880.000000	-nan	0.000000	0.000000	68727930880.000000	0.000000	68727930880.000000	0.000000	68727930880.000000	-107688084727940297628194314263640145920.000000	0.000000	0.000000	68727930880.000000	
	0.000000	14812040.000000	0.000000	68727930880.000000	-53189833242204364184572738541589626880.000000	0.000000	0.000000	83895360.000000	0.000000	261208778387488768.000000	0.000000	0.000000	-127626535088328441199257056088722243584.000000	0.000000	0.000000	0.000000	
**layer1-biases	: (8) 	.n: 1, .c: 1, .h: 8, .w: 1	(h:0x26ede70, d:0x7fd76e600e00)
<---- batch [0] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [1] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	17331160549995323848587739136.000000	0.000000	
<---- batch [2] ----->
	0.000000	0.000000	17331765012905131163175092224.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [3] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
[       OK ] Testlayer.DenseInitWeightBias (2 ms)
[ RUN      ] Testlayer.DenseLayerForwardExecute
**input_dataset	: (24) 	.n: 1, .c: 2, .h: 3, .w: 4	(h:0x2703ac0, d:0)
	0.5	0.549917	0.599335	0.64776	
	0.694709	0.739713	0.782321	0.822109	
	0.858678	0.891663	0.920735	0.945604	
	0.96602	0.981779	0.992725	0.998747	
	0.999787	0.995832	0.986924	0.97315	
	0.954649	0.931605	0.904248	0.872853	
.. initialized dense-layer-0 layer ..
invoke forward func
-3.80132e-26
4.58603e-41
-3.80132e-26
4.58603e-41
[       OK ] Testlayer.DenseLayerForwardExecute (1059 ms)
[ RUN      ] Testlayer.DenseLayerBackwardExecute
**grad_output	: (24) 	.n: 1, .c: 2, .h: 3, .w: 4	(h:0x1f220d10, d:0)
	0.5	0.549917	0.599335	0.64776	
	0.694709	0.739713	0.782321	0.822109	
	0.858678	0.891663	0.920735	0.945604	
	0.96602	0.981779	0.992725	0.998747	
	0.999787	0.995832	0.986924	0.97315	
	0.954649	0.931605	0.904248	0.872853	
**input	: (24) 	.n: 1, .c: 2, .h: 3, .w: 4	(h:0x1f8d1bc0, d:0)
	0.5	0.549917	0.599335	0.64776	
	0.694709	0.739713	0.782321	0.822109	
	0.858678	0.891663	0.920735	0.945604	
	0.96602	0.981779	0.992725	0.998747	
	0.999787	0.995832	0.986924	0.97315	
	0.954649	0.931605	0.904248	0.872853	
.. initialized dense-layer layer ..
Print Gradient Input 
**gradient input data	: (24) 	.n: 1, .c: 2, .h: 3, .w: 4	(h:0x1f221680, d:0x7fd76e62d800)
	0	0	2.5757e-40	0	
	3.43235e-20	0	3.43235e-20	0	
	0	0	0	0	
	1.75496e-37	0	0	0	
	7.17465e-43	0	1.75496e-37	0	
	0	0	0	0	
[       OK ] Testlayer.DenseLayerBackwardExecute (1 ms)
[ RUN      ] Testlayer.ActivationLayerFwdInit
**activation_fwd_input_data	: (30) 	.n: 1, .c: 2, .h: 3, .w: 5	(h:0x1f7cfdb0, d:0)
	0.5	0.549917	0.599335	0.64776	0.694709	
	0.739713	0.782321	0.822109	0.858678	0.891663	
	0.920735	0.945604	0.96602	0.981779	0.992725	
	0.998747	0.999787	0.995832	0.986924	0.97315	
	0.954649	0.931605	0.904248	0.872853	0.837732	
	0.799236	0.757751	0.71369	0.667494	0.619625	
**activation_fwd_output_data	: (30) 	.n: 1, .c: 2, .h: 3, .w: 5	(h:0x1f7d1260, d:0x7fd76e62d000)
	-3.80132e-26	4.58603e-41	-3.80132e-26	4.58603e-41	0	
	0	0	0	0	0	
	0	0	0	0	0	
	0	0	0	0	0	
	0	0	0	0	0	
	0	0	0	1.79366e-43	0	
[       OK ] Testlayer.ActivationLayerFwdInit (0 ms)
[ RUN      ] Testlayer.ActivationLayerBwdExecute
**forward_output_data	: (144) 	.n: 1, .c: 2, .h: 8, .w: 9	(h:0x1f844e00, d:0x7fd76e659800)
	-3.80132e-26	4.58603e-41	-3.80132e-26	4.58603e-41	0	0	0	0	8.4856e-38	
	4.00543e-05	-2.65891e+37	2.93443e-39	7.34331e-40	-6.31543e+28	8.68926e-35	3.9978e+07	9.29155e-35	3.77569e+07	
	-3.72287e+37	2.91147e-39	6.43572e-40	-3.40627e+28	6.48229e-40	2.36223e+10	6.48229e-40	2.36223e+10	-nan	
	2.89282e-39	6.42869e-40	-2.36118e+21	-nan	-8.85732e+20	6.48229e-40	2.36223e+10	-1.27627e+38	2.91721e-39	
	1.23978e-38	7.97071e+07	1.37674e-34	7.97071e+07	2.11438e-39	3.38277e-06	-4.25456e+37	2.93443e-39	-nan	
	2.64486e-38	-nan	2.49735e-38	-9.90354e+27	-8.85444e+20	-7.38013e+20	2.91578e-39	3.58159e-39	7.97071e+07	
	1.57957e-37	-4.95176e+29	5.05312e-39	1.50463e-36	-4.25447e+37	2.59579e-39	6.61732e-22	-8.91317e+28	2.1133e-39	
	4.76837e-05	1.10937e-37	-4.95176e+29	-5.31794e+37	2.89425e-39	2.43603e-29	6.44616e+16	-nan	2.64486e-38	
	2.43561e-29	3.4234e+17	-1.88931e+23	2.47851e-24	2.43561e-29	3.4234e+17	6.60426e-40	5.04403e+17	5.05382e-39	
	3.60343e+16	-5.38441e+37	2.91578e-39	5.05383e-39	3.61692e+16	7.99256e-39	4.95408e+16	-nan	2.49735e-38	
	-2.65897e+37	2.93443e-39	1.51116e+23	-8.85444e+20	2.11617e-39	3.41257e-06	128	-8.85444e+20	-4.25456e+37	
	2.93443e-39	3.58554e-39	3.41257e-06	2.19832e-39	2.35099e-38	8.67363e-19	-8.85444e+20	-5.51733e+37	4.81012e-34	
	5.05384e-39	3.60343e+16	3.15544e-30	-1.32817e+21	5.05383e-39	3.61692e+16	-5.31795e+37	2.91004e-39	7.99257e-39	
	4.95452e+16	1.25666e-34	1.54674e+07	-nan	2.49735e-38	-nan	2.89425e-39	-nan	2.64486e-38	
	2.10195e-44	-6.14018e+29	8.48704e-38	4.00543e-05	-7.97666e+36	2.89569e-39	2.43565e-29	4.58211e-05	7.48702e-35	
	3.9978e+07	8.08814e-35	3.77569e+07	-3.32369e+36	2.92295e-39	6.44286e-40	-3.28248e+28	6.32093e-35	2.82465e+07	
**backward_grad_input_data	: (144) 	.n: 1, .c: 2, .h: 8, .w: 9	(h:0x1f845090, d:0x7fd76e65a000)
	-3.80132e-26	4.58603e-41	-3.80132e-26	4.58603e-41	0	0	0	0	3.58519e-39	
	3.77487e+07	7.88861e-31	-1.32817e+21	1.25672e-34	1.54674e+07	-3.32372e+36	2.92295e-39	1.2326e-32	-8.85444e+20	
	1.82753e-38	3.4234e+17	2.43565e-29	4.58211e-05	-8.05973e+36	2.89569e-39	8.48618e-38	4.00543e-05	7.48614e-35	
	3.9978e+07	8.08873e-35	3.77569e+07	-1.27626e+38	2.89578e-39	6.45725e-40	-3.28248e+28	6.32005e-35	2.82465e+07	
	6.9219e-35	3.13291e-23	-7.44468e+37	2.91721e-39	7.38277e-40	-3.28248e+28	1.4063e-07	-5.94211e+29	3.5834e-39	
	1.00213e+17	-2.65897e+37	1.8773e-36	7.99041e-39	2.52217e+17	3.76706e-39	1.06405e+17	1.03789e-38	6.36172e+16	
	-nan	2.91578e-39	1.09299e-38	3.60288e+17	6.42869e-40	-6.14018e+29	3.58159e-39	3.4234e+17	-1.27627e+38	
	2.89425e-39	6.42869e-40	-6.14018e+29	7.98969e-39	7.97071e+07	5.05167e-39	1.72282e+17	-7.97666e+36	2.91578e-39	
	3.66769e-39	2.35099e-38	3.5823e-39	1.00676e+08	1.25643e-34	1.54674e+07	-nan	2.89425e-39	1.97215e-31	
	-1.32817e+21	1.2326e-32	-8.85444e+20	8.48589e-38	4.00543e-05	-7.97666e+36	2.89569e-39	2.43565e-29	4.58211e-05	
	7.48585e-35	3.9978e+07	8.08873e-35	3.77569e+07	-3.32369e+36	2.92295e-39	6.4573e-40	-3.28248e+28	6.31976e-35	
	2.82465e+07	3.5816e-39	3.4234e+17	-1.62291e+33	2.92008e-39	6.92161e-35	3.13291e-23	7.38288e-40	-3.28248e+28	
	1.4063e-07	-5.94211e+29	-2.12728e+37	2.91147e-39	7.99258e-39	1.00213e+17	1.23996e-38	2.52217e+17	8.17839e-39	
	1.06405e+17	-4.28779e+37	2.93443e-39	1.03817e-38	6.36172e+16	1.09327e-38	3.60288e+17	6.42869e-40	-6.14018e+29	
	-nan	2.91578e-39	7.98971e-39	3.4234e+17	6.42869e-40	-6.14018e+29	3.58232e-39	3.77487e+07	-2.65897e+37	
	2.93443e-39	3.9443e-31	-1.32817e+21	1.25672e-34	1.54674e+07	2.46519e-32	-8.85444e+20	-5.31793e+37	2.90143e-39	
[       OK ] Testlayer.ActivationLayerBwdExecute (1 ms)
[----------] 5 tests from Testlayer (1064 ms total)

[----------] Global test environment tear-down
[==========] 8 tests from 2 test suites ran. (1554 ms total)
[  PASSED  ] 8 tests.
