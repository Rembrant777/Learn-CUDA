Running main() from /home/raytrack/workspace/Learn-CUDA/Chapter10/googletest/googletest/src/gtest_main.cc
[==========] Running 9 tests from 2 test suites.
[----------] Global test environment set-up.
[----------] 3 tests from TestLayer
[ RUN      ] TestLayer.DenseLayerCreateTest
[       OK ] TestLayer.DenseLayerCreateTest (0 ms)
[ RUN      ] TestLayer.ActivationLayerCreateTest
[       OK ] TestLayer.ActivationLayerCreateTest (0 ms)
[ RUN      ] TestLayer.SoftmaxLayerCreateTest
[       OK ] TestLayer.SoftmaxLayerCreateTest (500 ms)
[----------] 3 tests from TestLayer (501 ms total)

[----------] 6 tests from Testlayer
[ RUN      ] Testlayer.DenseInitWeightBias
.. initialized dense-layer-0 layer ..
**bias-value	: (8) 	.n: 1, .c: 1, .h: 8, .w: 1	(h:0x2401710, d:0x7feff6600600)
<---- batch [0] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [1] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [2] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [3] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
**weight-value	: (192) 	.n: 1, .c: 1, .h: 24, .w: 8	(h:0x2401290, d:0x7feff6600200)
<---- batch [0] ----->
	0.497185	0.432557	-0.371876	0.499041	-0.263911	-0.103419	-0.112089	0.169746	0.435539	0.346311	-0.186726	0.024548	-0.056547	-0.270423	0.034414	0.413962	
	-0.042795	-0.069301	0.439128	0.278389	0.215971	0.302758	-0.407199	0.018153	0.365020	0.329147	0.329603	-0.226950	-0.440757	0.170528	0.093066	0.171654	
	-0.088212	-0.302449	-0.210370	-0.357880	0.283314	-0.087461	-0.465829	0.124030	0.160636	-0.201505	-0.053865	-0.277875	-0.426636	-0.030761	-0.403828	0.403370	
	-0.380510	0.024799	-0.416377	0.416861	0.410448	-0.201070	0.084389	0.065912	0.113938	0.456536	-0.239021	-0.268985	0.033448	0.449938	-0.006940	0.040601	
<---- batch [1] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	-1135933780048264940247436387942400.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	18887681556968829662122133684224.000000	272293913881238044672.000000	0.000000	0.000000	-nan	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [2] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [3] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
.. saving dense-layer-0 parameter .. done ..
dense-layer-0load_parameter
.. loaded dense-layer-0 pretrain parameter..
**layer1-weights	: (192) 	.n: 1, .c: 1, .h: 24, .w: 8	(h:0x2416d90, d:0x7feff6600a00)
<---- batch [0] ----->
	0.497185	0.432557	-0.371876	0.499041	-0.263911	-0.103419	-0.112089	0.169746	0.435539	0.346311	-0.186726	0.024548	-0.056547	-0.270423	0.034414	0.413962	
	-0.042795	-0.069301	0.439128	0.278389	0.215971	0.302758	-0.407199	0.018153	0.365020	0.329147	0.329603	-0.226950	-0.440757	0.170528	0.093066	0.171654	
	-0.088212	-0.302449	-0.210370	-0.357880	0.283314	-0.087461	-0.465829	0.124030	0.160636	-0.201505	-0.053865	-0.277875	-0.426636	-0.030761	-0.403828	0.403370	
	-0.380510	0.024799	-0.416377	0.416861	0.410448	-0.201070	0.084389	0.065912	0.113938	0.456536	-0.239021	-0.268985	0.033448	0.449938	-0.006940	0.040601	
<---- batch [1] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	-1136092236373293468922623475843072.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	-nan	-nan	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	-0.426636	-0.030761	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [2] ----->
	0.000000	0.000000	0.000000	0.000000	-1135973394129522072416233159917568.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	-1135973703614531893761301884698624.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [3] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	14776145018880.000000	0.000000	0.000000	0.000000	-453420392871028475297792.000000	0.000000	0.000000	163255486492180480.000000	0.000000	414331165718085632.000000	-0.000000	0.000206	-8059736460407685045239455443515867136.000000	0.000000	0.000000	1048785.000000	
	0.000000	68727930880.000000	0.000000	68727930880.000000	-nan	0.000000	0.000000	68727930880.000000	0.000000	68727930880.000000	0.000000	68727930880.000000	-107688084727940297628194314263640145920.000000	0.000000	0.000000	68727930880.000000	
**layer1-biases	: (8) 	.n: 1, .c: 1, .h: 8, .w: 1	(h:0x2413270, d:0x7feff6600e00)
<---- batch [0] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [1] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [2] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [3] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
[       OK ] Testlayer.DenseInitWeightBias (1 ms)
[ RUN      ] Testlayer.DenseLayerForwardExecute
**input_dataset	: (24) 	.n: 1, .c: 2, .h: 3, .w: 4	(h:0x2417330, d:0)
	0.5	0.549917	0.599335	0.64776	
	0.694709	0.739713	0.782321	0.822109	
	0.858678	0.891663	0.920735	0.945604	
	0.96602	0.981779	0.992725	0.998747	
	0.999787	0.995832	0.986924	0.97315	
	0.954649	0.931605	0.904248	0.872853	
.. initialized dense-layer-0 layer ..
invoke forward func
4.66318e-21
4.58953e-41
4.66318e-21
4.58953e-41
[       OK ] Testlayer.DenseLayerForwardExecute (1140 ms)
[ RUN      ] Testlayer.DenseLayerBackwardExecute
**grad_output	: (24) 	.n: 1, .c: 2, .h: 3, .w: 4	(h:0x1f5b89d0, d:0)
	0.5	0.549917	0.599335	0.64776	
	0.694709	0.739713	0.782321	0.822109	
	0.858678	0.891663	0.920735	0.945604	
	0.96602	0.981779	0.992725	0.998747	
	0.999787	0.995832	0.986924	0.97315	
	0.954649	0.931605	0.904248	0.872853	
**input	: (24) 	.n: 1, .c: 2, .h: 3, .w: 4	(h:0x1f657860, d:0)
	0.5	0.549917	0.599335	0.64776	
	0.694709	0.739713	0.782321	0.822109	
	0.858678	0.891663	0.920735	0.945604	
	0.96602	0.981779	0.992725	0.998747	
	0.999787	0.995832	0.986924	0.97315	
	0.954649	0.931605	0.904248	0.872853	
.. initialized dense-layer layer ..
Print Gradient Input 
**gradient input data	: (24) 	.n: 1, .c: 2, .h: 3, .w: 4	(h:0x1f5edf20, d:0x7feff662d800)
	0	0	2.5757e-40	0	
	4.71949e-20	0	4.71949e-20	0	
	0	0	0	0	
	1.41109e-37	0	0	0	
	7.17465e-43	0	1.41109e-37	0	
	0	0	0	0	
[       OK ] Testlayer.DenseLayerBackwardExecute (1 ms)
[ RUN      ] Testlayer.ActivationLayerFwdInit
**activation_fwd_input_data	: (30) 	.n: 1, .c: 2, .h: 3, .w: 5	(h:0x1f61e810, d:0)
	0.5	0.549917	0.599335	0.64776	0.694709	
	0.739713	0.782321	0.822109	0.858678	0.891663	
	0.920735	0.945604	0.96602	0.981779	0.992725	
	0.998747	0.999787	0.995832	0.986924	0.97315	
	0.954649	0.931605	0.904248	0.872853	0.837732	
	0.799236	0.757751	0.71369	0.667494	0.619625	
**activation_fwd_output_data	: (30) 	.n: 1, .c: 2, .h: 3, .w: 5	(h:0x1f61fc90, d:0x7feff662d000)
	4.66318e-21	4.58953e-41	4.66318e-21	4.58953e-41	0	
	0	0	0	0	0	
	0	0	0	0	0	
	0	0	0	0	0	
	0	0	0	0	0	
	0	0	0	0	0	
[       OK ] Testlayer.ActivationLayerFwdInit (1 ms)
[ RUN      ] Testlayer.ActivationLayerBwdExecute
**forward_output_data	: (144) 	.n: 1, .c: 2, .h: 8, .w: 9	(h:0x1f8c1990, d:0x7feff6659800)
	4.66318e-21	4.58953e-41	4.66318e-21	4.58953e-41	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
**backward_grad_input_data	: (144) 	.n: 1, .c: 2, .h: 8, .w: 9	(h:0x1f8c1c20, d:0x7feff665a000)
	4.66318e-21	4.58953e-41	4.66318e-21	4.58953e-41	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	
[       OK ] Testlayer.ActivationLayerBwdExecute (1 ms)
[ RUN      ] Testlayer.DenseLayerBackwardCalcuation
Softmax Layer Forward Accuracy 2
Softmax Layer Forward Loss 1.09861
grad input value 
**grad-input-data	: (3000) 	.n: 2, .c: 3, .h: 50, .w: 20	(h:0x1f9be440, d:0x7feff6682200)
<---- batch [0] ----->
	4.66398e-21	4.58953e-41	4.66398e-21	4.58953e-41	6.60226e-20	0	6.60226e-20	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
<---- batch [1] ----->
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
[       OK ] Testlayer.DenseLayerBackwardCalcuation (2 ms)
[----------] 6 tests from Testlayer (1147 ms total)

[----------] Global test environment tear-down
[==========] 9 tests from 2 test suites ran. (1649 ms total)
[  PASSED  ] 9 tests.
