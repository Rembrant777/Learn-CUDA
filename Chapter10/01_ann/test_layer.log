Running main() from /home/raytrack/workspace/Learn-CUDA/Chapter10/googletest/googletest/src/gtest_main.cc
[==========] Running 4 tests from 2 test suites.
[----------] Global test environment set-up.
[----------] 3 tests from TestLayer
[ RUN      ] TestLayer.DenseLayerCreateTest
[       OK ] TestLayer.DenseLayerCreateTest (0 ms)
[ RUN      ] TestLayer.ActivationLayerCreateTest
[       OK ] TestLayer.ActivationLayerCreateTest (0 ms)
[ RUN      ] TestLayer.SoftmaxLayerCreateTest
[       OK ] TestLayer.SoftmaxLayerCreateTest (519 ms)
[----------] 3 tests from TestLayer (520 ms total)

[----------] 1 test from Testlayer
[ RUN      ] Testlayer.DenseInitWeightBias
.. initialized dense-layer-0 layer ..
**bias-value	: (8) 	.n: 1, .c: 1, .h: 8, .w: 1	(h:0x13cc910, d:0x7ff846600600)
<---- batch [0] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [1] ----->
	0.000000	0.000000	0.000000	0.000000	30539281953378196331662671872.000000	0.000000	0.000000	0.000000	
<---- batch [2] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [3] ----->
	0.000000	0.000000	0.000000	0.000000	30539600713115790032714596352.000000	0.000000	30539600713115790032714596352.000000	0.000000	
**weight-value	: (192) 	.n: 1, .c: 1, .h: 24, .w: 8	(h:0x13ccb50, d:0x7ff846600200)
<---- batch [0] ----->
	0.497185	0.432557	-0.371876	0.499041	-0.263911	-0.103419	-0.112089	0.169746	0.435539	0.346311	-0.186726	0.024548	-0.056547	-0.270423	0.034414	0.413962	
	-0.042795	-0.069301	0.439128	0.278389	0.215971	0.302758	-0.407199	0.018153	0.365020	0.329147	0.329603	-0.226950	-0.440757	0.170528	0.093066	0.171654	
	-0.088212	-0.302449	-0.210370	-0.357880	0.283314	-0.087461	-0.465829	0.124030	0.160636	-0.201505	-0.053865	-0.277875	-0.426636	-0.030761	-0.403828	0.403370	
	-0.380510	0.024799	-0.416377	0.416861	0.410448	-0.201070	0.084389	0.065912	0.113938	0.456536	-0.239021	-0.268985	0.033448	0.449938	-0.006940	0.040601	
<---- batch [1] ----->
	0.000000	0.000000	0.000000	0.000000	30539279592194954896840065024.000000	0.000000	30539751828843241861361434624.000000	0.000000	0.000000	0.000000	14337.500000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	12288.000000	0.000000	0.000000	0.000000	12288.000000	0.000000	0.000000	0.000000	0.000000	0.000000	12288.000000	0.000000	
<---- batch [2] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	144117284019896320.000000	0.000000	0.000000	0.000000	-53179347236439276270963557812674756608.000000	0.000000	0.000000	0.000000	
<---- batch [3] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
.. saving dense-layer-0 parameter .. done ..
dense-layer-0load_parameter
.. loaded dense-layer-0 pretrain parameter..
**layer1-weights	: (192) 	.n: 1, .c: 1, .h: 24, .w: 8	(h:0x13e04f0, d:0x7ff846600a00)
<---- batch [0] ----->
	0.497185	0.432557	-0.371876	0.499041	-0.263911	-0.103419	-0.112089	0.169746	0.435539	0.346311	-0.186726	0.024548	-0.056547	-0.270423	0.034414	0.413962	
	-0.042795	-0.069301	0.439128	0.278389	0.215971	0.302758	-0.407199	0.018153	0.365020	0.329147	0.329603	-0.226950	-0.440757	0.170528	0.093066	0.171654	
	-0.088212	-0.302449	-0.210370	-0.357880	0.283314	-0.087461	-0.465829	0.124030	0.160636	-0.201505	-0.053865	-0.277875	-0.426636	-0.030761	-0.403828	0.403370	
	-0.380510	0.024799	-0.416377	0.416861	0.410448	-0.201070	0.084389	0.065912	0.113938	0.456536	-0.239021	-0.268985	0.033448	0.449938	-0.006940	0.040601	
<---- batch [1] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	30545475337020479871360434176.000000	0.000000	0.000000	0.000000	14339.500000	0.000000	0.000000	0.000000	0.000000	0.000000	
	-nan	-nan	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	-0.426636	-0.030761	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [2] ----->
	0.000000	0.000000	0.000000	0.000000	14338.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	14338.003906	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [3] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	30539562934183927075552886784.000000	0.000000	
	0.000000	49542447759360000.000000	0.000000	0.000048	-127626393111461215637564088457963241472.000000	0.000000	0.000000	0.000040	0.000000	37879808.000000	0.000000	37756928.000000	-811448728142151374067969824915456.000000	0.000000	0.000000	-32824753854176411351622090752.000000	
	0.000000	-32824753854176411351622090752.000000	0.000000	-32824753854176411351622090752.000000	-7975480161787615435724874819915743232.000000	0.000000	0.000000	-32824753854176411351622090752.000000	0.000000	37879808.000000	0.000000	37756928.000000	-811448728142151374067969824915456.000000	0.000000	0.000000	-32824753854176411351622090752.000000	
**layer1-biases	: (8) 	.n: 1, .c: 1, .h: 8, .w: 1	(h:0x13dc8b0, d:0x7ff846600e00)
<---- batch [0] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [1] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [2] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
<---- batch [3] ----->
	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	
[       OK ] Testlayer.DenseInitWeightBias (1 ms)
[----------] 1 test from Testlayer (1 ms total)

[----------] Global test environment tear-down
[==========] 4 tests from 2 test suites ran. (522 ms total)
[  PASSED  ] 4 tests.
